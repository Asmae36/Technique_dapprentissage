# -*- coding: utf-8 -*-
"""Projet_TA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KDQlHuYSdJyBVZbPVmiusI_zbz7P1G4e
"""

#connection au drive

from google.colab import drive
drive.mount('/content/drive')

#bibliothèques dont nous aurons besoin

import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

#lire les données en dataframes

train= pd.read_csv('/content/drive/MyDrive/data/train.csv')
test = pd.read_csv('/content/drive/MyDrive/data/test.csv')

#visualiser les 10 premières lignes de la dataframe train

train.head(10)

#Apperçu des shape de train et test

print(train.shape)
print(test.shape)

#nombre de classes distinctes

train['species'].nunique()

"""## DATA PREPROCESSING"""

#importations utiles

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedShuffleSplit

# Label Encoding function

def Labels_encode(train,test):
    L_encoder = LabelEncoder().fit(train.species)
    Train = train.drop(['species','id'],axis = 1) 
    Test = test.drop(['id'],axis = 1)
    Labels = L_encoder.transform(train.species)
    Ids_Test = test.id
    Classes = list(L_encoder.classes_) 
    return Train, Test,Labels,Ids_Test, Classes 

Train, Test,Labels,Ids_Test, Classes = Labels_encode(train,test)

Train.head()

#Le shape change du train vu précédemment car nous avons droppé la colonne 'id' et 'species'
Train.shape

#Après encoding voici les labels dont nous disposons
Labels

#Ici nous avons procéder à la stratification
X = Train.values
Y = Labels
Strat = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=30)
Strat.get_n_splits(X, Y)

for Training_indice,Testing_indice in Strat.split(X, Y):
    X_Train, X_Test = X[Training_indice], X[Testing_indice]
    Y_Train, Y_Test = Y[Training_indice], Y[Testing_indice]

# importing libraries
models = [
    KNeighborsClassifier(n_neighbors=1,weights='distance',algorithm='ball_tree',leaf_size=10,p=1),
    SVC(kernel='linear',C = 100,gamma=0.0001),
    RandomForestClassifier(n_estimators=350,max_depth=30),
    GaussianNB(var_smoothing=0.002),
    AdaBoostClassifier(n_estimators=70,learning_rate=0.01),
    MLPClassifier(hidden_layer_sizes=90,learning_rate_init=0.01,solver='adam',activation='tanh'),
    SGDClassifier(loss='hinge',penalty= 'l1')
]

col=["Model", "Accuracy"] 
L = pd.DataFrame(columns=col)

for c_model in models:
    c_model.fit(X_Train, Y_Train)  
    name = c_model.__class__.__name__
    training_predictions = c_model.predict(X_Test)   
    accuracy = accuracy_score(Y_Test, training_predictions) 
    print(name+" " +"Accuracy is: {:.2%}".format(accuracy))
    
    df_L = pd.DataFrame([[name, accuracy*100]], columns=col) 
    L = L.append(df_L)

import seaborn as sns
import matplotlib.pyplot as plt

sns.barplot(y='Model', x='Accuracy', data=L, color="purple")

plt.xlabel('Accuracy %')
plt.title('Classifier Accuracy')
plt.show()

L.head(6)

